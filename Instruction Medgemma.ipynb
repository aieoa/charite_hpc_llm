{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# LLM Usage on Cluster\n",
    "\n",
    "Steps to download and install a huggingface model locally in a node. Assume you want to use [medgemma-27b-text-it](https://huggingface.co/google/medgemma-27b-text-it), which occupies around **60-80 GB** solid state storage. Given a typical quota of **100 GB** this means that you can only install one model at a time. \n",
    "You can check your usage with `df -h ~/` in terminal while being logged into one of the frontend notes (`s-sc-frontent1-3.charite.de`). \n",
    "In general shell commands can be executed in a terminal while being on the cluster (also available in the JupyterHub, scroll down) or can be executed in this notebook by prepending \"!\". E.g., to create directory you can execute `mkdir new_dir` in terminal or you can create a new code cell in this notebook and write `!mkdir new_dir` and run the cell.\n",
    "\n",
    "## Preliminary\n",
    "\n",
    "Get your API key from Huggingface. By creating an account on [Huggingface](https://huggingface.co/) you should be able to create an access token in your account under the tab `Access Tokens`. \n",
    "\n",
    "\n",
    "## 1. Login to Cluster\n",
    "\n",
    "### Pull this Notebook from Github\n",
    "\n",
    "Login into one of three frontend node by specifying your username and a frontend node ID in [1,2,3] with terminal (Powershell on Windows):\n",
    "```shell\n",
    "ssh <usr>@s-sc-frontend[1-3].charite.de\n",
    "````\n",
    "Then clone the repository containing this notebook:\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Go to the [Jupyterhub](https://git.bihealth.org/charite-sc-public/sc-wiki/-/wikis/Resources/User%20Documentation/User%20Guide:%20HPC%20@Charite#jupyterhub) which offers you pre-configured notebooks and shells. Select a server offering **Exclusive GPU**. Open a shell on the cluster and pull this notebook by cloning the repository.\n",
    "```shell\n",
    "mkdir -p git/charite_hpc_llm\n",
    "git clone https://github.com/aieoa/charite_hpc_llm.git\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 2. Create LLM-Specific Virtual Environment and Set Project Directory for Storage\n",
    "\n",
    "### Virtual Environment\n",
    "For reusability create and later simply load (activate) a virtual environment which contains all packages needed to run a specific model. For Medgemma, we will need packages provided in a pre-configured conda environment specific for GPU compute nodes plus additional modules. Complete list of pre-configured kernels can be found [here](https://git.bihealth.org/charite-sc-public/sc-wiki/-/wikis/Resources/User%20Documentation/User%20Guide:%20HPC%20@Charite#shared-conda-environments) and [yaml](https://git.bihealth.org/charite-sc-public/conda-envs/-/tree/main/)). \n",
    "\n",
    "As we will require newer versions of at least one package as provided in the pre-configured conda kernel conda_envs-gpulab (write-protected), we will clone the gpulab environment, install additional packages into the cloned one and create a kernel that can be selected for this notebook:\n",
    "\n",
    "#### Create Conda gpulab Clone\n",
    "Create clone, called `llm_env` or any other name and activate:\n",
    "```shell\n",
    "conda create --name llm_env --clone gpulab\n",
    "conda activate llm_env\n",
    "```\n",
    "\n",
    "#### Register Jupyter Kernel\n",
    "Register a Jupyter kernel with currently active environment \n",
    "```\n",
    "conda install ipykernel\n",
    "python -m ipykernel install --user --name llm_env --display-name \"Python (llm_env)\"\n",
    "```\n",
    "\n",
    "#### Install Medgemma-Specific Modules\n",
    "Install missing packages and force update for Jinja2\n",
    "\n",
    "```\n",
    "conda env update -f environment.yaml --prune\n",
    "```\n",
    "\n",
    "#### Start Jupyter Notebook with Registered Kernel\n",
    "Start notebook by selecting it from the filebrowser showing your home directory on the cluster. Then select the new environment: Listed top right in notebook and selectable in drop-down menue likely listed as `conda env:.conda-llm_env`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Project Directory\n",
    "On Charite's [service portal](https://s-m42-it-appl.charite.de/wm/app-SelfServicePortal/landing-page) you can apply for a project directory of eg 1 TB.\n",
    "Given that you are granted with a project directory (see email by bihealth), ascertain that model and conda environments point to it for not running into out of memory errors. Otherwise, for smaller models, you might be able to use a folder in the home directory if enough space is left (1 TB is user quota)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# otherwise, eg, use home directory \"~\"\n",
    "scratch_dir = \"<your_granted_scratch_dir_on_hpc_see_mail\" \n",
    "repo_id = \"google/medgemma-27b-text-it\"\n",
    "model_dir = Path(scratch_dir) / \"data\" / \"models\" / repo_id.split('/')[-1]\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# verify that llm_env is used and Jinja2>=3.1.x\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import jinja2\n",
    "assert jinja2.__version__.startswith(\"3.1.\")\n",
    "print(jinja2.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "## 4. Model Installation\n",
    "Install the model (attached file) by executing the next cell. Note, this has to be done only once and is relatively time-consuming. Never share your notebook with your plaintext key. A better habit is to set it as an environment variable, say `HF_TOKEN`, and query it with `os.environ[\"HF_TOKEN\"]`.\n",
    "\n",
    "### a) Terminal\n",
    "\n",
    "```shell\n",
    "python scripts/download_model.py --repo_id \"google/medgemma-27b-text-it\" --local_dir <model_dir> --token $HF_TOKEN\n",
    "```\n",
    "\n",
    "### b) Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Option a) unsafe: copy-paste HuggingFace token here\n",
    "token=\"<your_secret_HF_token>\"\n",
    "\n",
    "# Option b) set with 'export HF_TOKEN=\"secret_token\"' in ~/.bashrc followed by \n",
    "# source ~/.bashrc to take immediate effect then load environment variable with\n",
    "token=os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "assert len(token) > 3, \"ERROR\\tToken not set!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "## Download and install medgemma once by decommenting following line\n",
    "## Accidently relaunching does not trigger another download if model_dir has not changed\n",
    "snapshot_download(\n",
    "    repo_id=repo_id,\n",
    "    local_dir=model_dir,\n",
    "    max_workers=4,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "If you encounter problems with your quota (you should have 100 GB), check for bulky folders and delete unused stuff via `du -h --max-depth=1 ~ | sort -hr | head -n 10`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!df -h $scratch_dir\n",
    "!du -sh $scratch_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 6. Prompt Completion\n",
    "\n",
    "Organize your code to load model only once as this is a true bottleneck and then loop over prompts and collect completions.\n",
    "\n",
    "If the repo must be fetched again, redirect the Hub cache to a larger mount before any imports:\n",
    "export HF_HUB_CACHE=/large_mount/hf_cache  Or in Python prior to importing transformers:\n",
    "import os; os.environ“HF_HUB_CACHE” = “/large_mount/hf_cache”  This is the recommended fix when “No space left on device” arises due to small root partitions ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Important: point here to downloaded version to avoid repeated caching under ~/.cache of the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, dtype=\"auto\", device_map=\"auto\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "# this may take 10-15 minutes\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Automize Queries\n",
    "\n",
    "### Push Data from PC to Cluster\n",
    "\n",
    "Assume text files are located in your home directory under `data` and you want to copy it to your home directory on the cluster. If the folder does not exist yet on the cluster. Ideally, you would use the granted project directory. If not available (yet, use instead your home directory: `~`).\n",
    "```shell\n",
    "ssh <usr>@s-sc-frontend[1-3].charite.de \"mkdir -p <scratch_dir>/data/input\"\n",
    "# or stored in home directory\n",
    "ssh <usr>@s-sc-frontend[1-3].charite.de \"mkdir -p ~/data/input\"\n",
    "```\n",
    "Then copy all files recursively. If data is large, compression into single file is recommended.\n",
    "```shell\n",
    "scp -r ./data <usr>@s-sc-frontend[1-3].charite.de:<scratch_dir>/data/\n",
    "# or copied to home directory\n",
    "scp -r ./data <usr>@s-sc-frontend[1-3].charite.de:~/data/\n",
    "```\n",
    "\n",
    "Now assume, we want to iterate over the texts representing single patient cases and ask the same question, eg, \"Is the patient depressed?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Needs to be loaded only once\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Important: point here to downloaded version to avoid repeated caching under ~/.cache of the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "reports_dir = Path(scratch_dir) / \"data\" / \"input\"\n",
    "# if scratch_dir not available:\n",
    "# reports_dir = Path(os.path.expanduser(\"~\")) / \"data\" / \"input\"\n",
    "log_dir = Path(scratch_dir) / \"data\" / \"output\"\n",
    "os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set here the maximum number of tokens to be generated to a reasonable number to restrict output length\n",
    "# 1 word ~ 2.5 token\n",
    "max_new_token = 500 \n",
    "\n",
    "# Iterate over text files and store responses\n",
    "for filename in reports_dir.iterdir():\n",
    "\n",
    "    if not filename.name.endswith(\".txt\"): # filter for text files\n",
    "        continue\n",
    "    text_id = filename.stem\n",
    "    with open(reports_dir / filename) as fr:\n",
    "        text = fr.read()\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Given the record of a fictional patient: {text}. Is the patient depressed?\"\n",
    "            },\n",
    "        ]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "        \tmessages,\n",
    "        \tadd_generation_prompt=True,\n",
    "        \ttokenize=True,\n",
    "        \treturn_dict=True,\n",
    "        \treturn_tensors=\"pt\",\n",
    "        ).to(model.device)\n",
    "        \n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_token)\n",
    "        print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))        \n",
    "        answer = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
    "        logfile = log_dir / f\"{text_id}.log\"\n",
    "        with open(logfile, \"w\") as fw:\n",
    "            fw.write(answer)\n",
    "        print(f\"STATUS\\tOutput written to {logfile}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-llm_env]",
   "language": "python",
   "name": "conda-env-.conda-llm_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
