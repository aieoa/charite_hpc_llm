{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Instructions Mental Alpaca and Flan XXL\n",
    "\n",
    "Instructions for \n",
    "- [Mental Flan T5 XXL](https://huggingface.co/NEU-HAI/mental-flan-t5-xxl)\n",
    "- [Mental Alpaca](https://huggingface.co/NEU-HAI/mental-alpaca)\n",
    "\n",
    "Note, conversational chats are not possible with mental-alpaca. Task instructions need to follow specific formats, which you need to explore.\n",
    "\n",
    "More detailed instructions and how to loop and log over queries can be found in the `Instruction Medgemma` notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Create LLM-Specific Virtual Environment\n",
    "\n",
    "### Virtual Environment\n",
    "For reusability create and later simply load (activate) a virtual environment which contains all packages needed to run a specific model. We will need packages provided in a pre-configured conda environment specific for GPU compute nodes plus additional modules. Complete list of pre-configured kernels can be found [here](https://git.bihealth.org/charite-sc-public/sc-wiki/-/wikis/Resources/User%20Documentation/User%20Guide:%20HPC%20@Charite#shared-conda-environments) and [yaml](https://git.bihealth.org/charite-sc-public/conda-envs/-/tree/main/)). \n",
    "\n",
    "As we will require newer versions of at least one package as provided in the pre-configured conda kernel conda_envs-gpulab (write-protected), we will clone the gpulab environment, install additional packages into the cloned one and create a kernel that can be selected for this notebook:\n",
    "\n",
    "#### Create Conda gpulab Clone\n",
    "Create clone, called `llm_env` or any other name and activate:\n",
    "```shell\n",
    "conda create --name llm_env --clone gpulab\n",
    "conda activate llm_env\n",
    "```\n",
    "\n",
    "#### Register Jupyter Kernel\n",
    "Register a Jupyter kernel with currently active environment \n",
    "```\n",
    "conda install ipykernel\n",
    "python -m ipykernel install --user --name llm_env --display-name \"Python (llm_env)\"\n",
    "```\n",
    "\n",
    "#### Install Medgemma-Specific Modules\n",
    "Install missing packages and force update for Jinja2\n",
    "\n",
    "```\n",
    "conda env update -f environment.yaml --prune\n",
    "```\n",
    "\n",
    "#### Start Jupyter Notebook with Registered Kernel\n",
    "Start notebook by selecting it from the filebrowser showing your home directory on the cluster. Then select the new environment: Listed top right in notebook and selectable in drop-down menue likely listed as `conda env:.conda-llm_env`.\n",
    "\n",
    "### Update Torch\n",
    "Run in Terminal after activating `llm_env` environment:\n",
    "\n",
    "```shell\n",
    "pip install --upgrade --force-reinstall --no-cache-dir torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124 --extra-index-url https://pypi.org/simple\n",
    "```\n",
    "Followed by a downgrade to avoid incompatibilities:\n",
    "```shell\n",
    "pip install \"numpy<2.0\"\n",
    "```\n",
    "Note, this update requires Python processes like this notebook book to be shut down. Otherwise a lock is hold on module files that are to be replaced by above process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 2. Set Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# otherwise, eg, use home directory \"~\"\n",
    "scratch_dir = \"<your_granted_scratch_dir_on_hpc_see_mail\"\n",
    "assert os.path.isdir(scratch_dir)\n",
    "repo_id = \"NEU-HAI/mental-flan-t5-xxl\"\n",
    "# repo_id = \"NEU-HAI/mental-alpaca\"\n",
    "model_dir = Path(scratch_dir) / \"data\" / \"models\" / repo_id.split('/')[-1]\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Set HF Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option a) unsafe: copy-paste HuggingFace token here\n",
    "token=\"<your_secret_HF_token>\"\n",
    "\n",
    "# Option b) set with 'export HF_TOKEN=\"secret_token\"' in ~/.bashrc followed by \n",
    "# source ~/.bashrc to take immediate effect then load environment variable with\n",
    "token=os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "assert len(token) > 3, \"ERROR\\tToken not set!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Download Model\n",
    "\n",
    "Due to subprocess issues I ran when trying to download with multiple workers from a notebook, I recommend to launch the download separately in a terminal. First set arguments for the script in terminal, then call the provided download_model script:\n",
    "```shell\n",
    "conda activate llm_env\n",
    "repo_id=\"NEU-HAI/mental-flan-t5-xxl\" \n",
    "# or repo_id=\"NEU-HAI/mental-alpaca\"\n",
    "\n",
    "local_dir=\"<your_scratch_dir>\"\n",
    "token=$HF_TOKEN # or direct setting\n",
    "```\n",
    "Now, trigger the download:\n",
    "```shell\n",
    "python scripts/download_model.py --repo_id $repo_id --local_dir $local_dir --token $token\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_dir_str=str(model_dir.resolve())\n",
    "\n",
    "print(\"Files in model directory:\")\n",
    "for file in os.listdir(model_dir_str):\n",
    "    print(f\"{file}\")\n",
    "\n",
    "## you should see files like `config.json`, `tokenizer_config.json`, \n",
    "## `spiece.model` or `tokenizer.json`, `pytorch_model.bin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "\n",
    "print(\"numpy version\\t\", numpy.__version__)\n",
    "print(\"torch version\\t\", torch.__version__)\n",
    "\n",
    "assert numpy.__version__.startswith(\"1.\")\n",
    "assert torch.__version__.startswith(\"2.6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "model_dir_str=str(model_dir.resolve())\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir_str)\n",
    "\n",
    "# Load model directly\n",
    "match(repo_id):\n",
    "    case \"NEU-HAI/mental-alpaca\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_dir_str, device_map=\"auto\", dtype=\"auto\")\n",
    "    case \"NEU-HAI/mental-flan-t5-xxl\":\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_dir_str, device_map=\"auto\", dtype=\"auto\")\n",
    "\n",
    "# Example input prompt (Mental FLAN expects normal text string, not chat template)\n",
    "prompt = (\n",
    "    \"Classify the following text by the author's mental health risk:\\n\\n\"\n",
    "    \"Text: I feel constantly anxious and can't focus on my work lately.\\n\\n\"\n",
    "    \"Prediction:\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "t1 = time.time()\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "t2 = time.time()\n",
    "print(response)\n",
    "time_elapsed = str(timedelta(seconds=t2-t1))\n",
    "print(f\"response time: {time_elapsed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Trouble-Shooting\n",
    "\n",
    "### Memory Offloading \n",
    "\n",
    "1. Warnings like `Some parameters are on the meta device because they were offloaded to the cpu.` indicate that GPU memory (max. 40 GB) was insufficient to load complete model into GPU RAM. If warning turns into error, consider lowering precision by setting explicitly `torch_dtype=\"float16\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_env)",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
